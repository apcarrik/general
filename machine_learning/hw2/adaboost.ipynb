{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "adaboost.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5-rC-4moM3S"
      },
      "source": [
        "# Boosting a decision stump\n",
        "\n",
        "The goal of this notebook is to implement your own boosting module.\n",
        "\n",
        "* Go through an implementation of decision trees.\n",
        "* Implement Adaboost ensembling.\n",
        "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
        "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
        "* Explore the robustness of Adaboost to overfitting.\n",
        "\n",
        "*This file is adapted from course material by Carlos Guestrin and Emily Fox.*\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6DxWxi0oM3V"
      },
      "source": [
        "## Import some libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaWW7kIGoM3V"
      },
      "source": [
        "## please make sure that the packages are updated to the newest version. \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQq7bnHRoM3W"
      },
      "source": [
        "# Getting the data ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUFKWeqoM3X"
      },
      "source": [
        "Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoBeEgzooM3X"
      },
      "source": [
        "loans = pd.read_csv('loan_small.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF-dZJLFoM3X"
      },
      "source": [
        "### Recoding the target column\n",
        "\n",
        "We re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan. In the next cell, the features are also briefly explained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gn9x6cKoM3Y"
      },
      "source": [
        "features = ['grade',              # grade of the loan\n",
        "            'term',               # the term of the loan\n",
        "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
        "            'emp_length',         # number of years of employment\n",
        "           ]\n",
        "\n",
        "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
        "\n",
        "## please update pandas to the newest version in order to execute the following line\n",
        "loans.drop(columns=['loan_status'], inplace=True)\n",
        "\n",
        "target = 'safe_loans' # this variable will be used later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCCDPp-poM3Y"
      },
      "source": [
        "### Transform categorical data into binary features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AWgBY2coM3Z"
      },
      "source": [
        "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
        "\n",
        "We can do so with the following code block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M7uCoiToM3Z"
      },
      "source": [
        "loans = pd.get_dummies(loans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcdp9_lxoM3Z"
      },
      "source": [
        "Let's see what the feature columns look like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGDufmRoM3a",
        "outputId": "e058de86-2a8a-49ca-e883-944a0d5eeaed"
      },
      "source": [
        "features = list(loans.columns)\n",
        "features.remove('safe_loans')  # Remove the response variable\n",
        "features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['term_ 36 months',\n",
              " 'term_ 60 months',\n",
              " 'grade_A',\n",
              " 'grade_B',\n",
              " 'grade_C',\n",
              " 'grade_D',\n",
              " 'grade_E',\n",
              " 'grade_F',\n",
              " 'grade_G',\n",
              " 'home_ownership_MORTGAGE',\n",
              " 'home_ownership_NONE',\n",
              " 'home_ownership_OTHER',\n",
              " 'home_ownership_OWN',\n",
              " 'home_ownership_RENT',\n",
              " 'emp_length_1 year',\n",
              " 'emp_length_10+ years',\n",
              " 'emp_length_2 years',\n",
              " 'emp_length_3 years',\n",
              " 'emp_length_4 years',\n",
              " 'emp_length_5 years',\n",
              " 'emp_length_6 years',\n",
              " 'emp_length_7 years',\n",
              " 'emp_length_8 years',\n",
              " 'emp_length_9 years',\n",
              " 'emp_length_< 1 year']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAraLisoM3a"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiJ0_K3poM3a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Iw8k0pyoM3b"
      },
      "source": [
        "# Weighted decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qDfbH-FoM3b"
      },
      "source": [
        "Since the data weights change as we build an AdaBoost model, we need to first code a decision tree that supports weighting of individual data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twJD9n8joM3b"
      },
      "source": [
        "### Weighted error definition\n",
        "\n",
        "Consider a model with $N$ data points with:\n",
        "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
        "* Target $y_1 ... y_n$ \n",
        "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
        "\n",
        "Then the **weighted error** is defined by:\n",
        "$$\n",
        "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
        "$$\n",
        "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
        "\n",
        "\n",
        "### Write a function to compute weight of mistakes\n",
        "\n",
        "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
        "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
        "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
        "\n",
        "We are interested in computing the (total) weight of mistakes, i.e.\n",
        "$$\n",
        "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
        "$$\n",
        "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
        "$$\n",
        "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
        "$$\n",
        "\n",
        "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
        " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
        " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
        " \n",
        " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
        " \n",
        "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ji84PVdoM3c"
      },
      "source": [
        "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
        "    # Sum the weights of all entries with label +1\n",
        "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
        "    \n",
        "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
        "    ### YOUR CODE HERE\n",
        "    ...    \n",
        "    \n",
        "    # Sum the weights of all entries with label -1\n",
        "    ### YOUR CODE HERE\n",
        "    ...\n",
        "    \n",
        "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
        "    ### YOUR CODE HERE\n",
        "    ...\n",
        "    \n",
        "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
        "    #    class_label should be an integer of value +1 or -1.\n",
        "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
        "    ### YOUR CODE HERE\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKV3AZF7oM3c"
      },
      "source": [
        "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbHgEMrLoM3c",
        "outputId": "b02f59cb-7030-4d04-c5b7-546b2b74883e"
      },
      "source": [
        "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
        "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
        "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
        "    print('Test passed!')\n",
        "else:\n",
        "    print('Test failed... try again!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test failed... try again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17FnXUJToM3d"
      },
      "source": [
        "Recall that the **classification error** is defined as follows:\n",
        "$$\n",
        "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGxvIGqPoM3d"
      },
      "source": [
        "### Function to pick best feature to split on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJBxqVVPoM3d"
      },
      "source": [
        "The next step is to pick the best feature to split on.\n",
        "\n",
        "The **best_splitting_feature** function takes the data, the festures, the targetm and the data weights as input and returns the best feature to split on.\n",
        "  \n",
        "Complete the following function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9aSbft0oM3d"
      },
      "source": [
        "# If the data is identical in each feature, this function should return None\n",
        "\n",
        "def best_splitting_feature(data, features, target, data_weights):\n",
        "    \n",
        "    # These variables will keep track of the best feature and the corresponding error\n",
        "    best_feature = None\n",
        "    best_error = float('+inf') \n",
        "    num_points = float(len(data))\n",
        "\n",
        "    # Loop through each feature to consider splitting on that feature\n",
        "    for feature in features:\n",
        "        \n",
        "        # The left split will have all data points where the feature value is 0\n",
        "        # The right split will have all data points where the feature value is 1\n",
        "        left_split = data[data[feature] == 0]\n",
        "        right_split = data[data[feature] == 1]\n",
        "        \n",
        "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "                    \n",
        "        # Calculate the weight of mistakes for left and right sides\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # Compute weighted error by computing\n",
        "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # If this is the best error we have found so far, store the feature and the error\n",
        "        if error < best_error:\n",
        "            best_feature = feature\n",
        "            best_error = error\n",
        "    \n",
        "    # Return the best feature we found\n",
        "    return best_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4FTq7LaoM3d"
      },
      "source": [
        "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWdkKTrwoM3d",
        "outputId": "ec370f31-453b-4e2b-b401-ed368954c990"
      },
      "source": [
        "example_data_weights = np.array(len(train_data)* [1.5])\n",
        "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
        "    print('Test passed!')\n",
        "else:\n",
        "    print('Test failed... try again!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'error' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-63cf61174a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_data_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mbest_splitting_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_data_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'term_ 36 months'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test passed!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test failed... try again!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-14aba1e05e38>\u001b[0m in \u001b[0;36mbest_splitting_feature\u001b[0;34m(data, features, target, data_weights)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# If this is the best error we have found so far, store the feature and the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mbest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMOXXv0xoM3e"
      },
      "source": [
        "**Aside**. Relationship between weighted error and weight of mistakes:\n",
        "\n",
        "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
        "$$\n",
        "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
        "$$\n",
        "\n",
        "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
        "$$\n",
        "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
        "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
        "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}] + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
        "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
        "$$\n",
        "We then divide through by the total weight of all data points to obtain $\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})$:\n",
        "$$\n",
        "\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})\n",
        "= \\frac{\\mathrm{WM}({\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}({\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xaw0l5OoM3e"
      },
      "source": [
        "### Building the tree\n",
        "\n",
        "With the above functions implemented correctly, we are now ready to build our decision tree. A decision tree will be represented as a dictionary which contains the following keys:\n",
        "\n",
        "    { \n",
        "       'is_leaf'            : True/False.\n",
        "       'prediction'         : Prediction at the leaf node.\n",
        "       'left'               : (dictionary corresponding to the left tree).\n",
        "       'right'              : (dictionary corresponding to the right tree).\n",
        "       'features_remaining' : List of features that are posible splits.\n",
        "    }\n",
        "    \n",
        "Let us start with a function that creates a leaf node given a set of target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSYAlyifoM3e"
      },
      "source": [
        "def create_leaf(target_values, data_weights):\n",
        "    \n",
        "    # Create a leaf node\n",
        "    leaf = {'splitting_feature' : None,\n",
        "            'is_leaf': True}\n",
        "    \n",
        "    # Computed weight of mistakes.\n",
        "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
        "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
        "    ## YOUR CODE HERE\n",
        "    ...\n",
        "    \n",
        "    return leaf "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qOo-z0qoM3e"
      },
      "source": [
        "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
        "1. All data points in a node are from the same class.\n",
        "2. No more features to split on.\n",
        "3. Stop growing the tree when the tree depth reaches **max_depth**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwk8iJM2oM3e"
      },
      "source": [
        "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
        "    remaining_features = features[:] # Make a copy of the features.\n",
        "    target_values = data[target]\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
        "    \n",
        "    # Stopping condition 1. Error is 0.\n",
        "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
        "        print(\"Stopping condition 1 reached.\")                \n",
        "        return create_leaf(target_values, data_weights)\n",
        "    \n",
        "    # Stopping condition 2. No more features.\n",
        "    if remaining_features == []:\n",
        "        print(\"Stopping condition 2 reached.\")                \n",
        "        return create_leaf(target_values, data_weights)    \n",
        "    \n",
        "    # Additional stopping condition (limit tree depth)\n",
        "    if current_depth > max_depth:\n",
        "        print(\"Reached maximum depth. Stopping for now.\")\n",
        "        return create_leaf(target_values, data_weights)\n",
        "    \n",
        "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
        "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
        "    remaining_features.remove(splitting_feature)\n",
        "        \n",
        "    left_split = data[data[splitting_feature] == 0]\n",
        "    right_split = data[data[splitting_feature] == 1]\n",
        "    \n",
        "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
        "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
        "    \n",
        "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
        "              splitting_feature, len(left_split), len(right_split)))\n",
        "    \n",
        "    # Create a leaf node if the split is \"perfect\"\n",
        "    if len(left_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf(left_split[target], data_weights)\n",
        "    if len(right_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf(right_split[target], data_weights)\n",
        "    \n",
        "    # Repeat (recurse) on left and right subtrees\n",
        "    ## YOUR CODE HERE\n",
        "    \n",
        "    return {'is_leaf'          : False, \n",
        "            'prediction'       : None,\n",
        "            'splitting_feature': splitting_feature,\n",
        "            'left'             : left_tree, \n",
        "            'right'            : right_tree}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ut1sv4oM3f"
      },
      "source": [
        "Here is a recursive function to count the nodes in your tree:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk-0__mcoM3f"
      },
      "source": [
        "def count_nodes(tree):\n",
        "    if tree['is_leaf']:\n",
        "        return 1\n",
        "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCyY0avgoM3f"
      },
      "source": [
        "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUkAqpFroM3f"
      },
      "source": [
        "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
        "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
        "                                        example_data_weights, max_depth=2)\n",
        "if count_nodes(small_data_decision_tree) == 7:\n",
        "    print('Test passed!')\n",
        "else:\n",
        "    print('Test failed... try again!')\n",
        "    print('Number of nodes found:', count_nodes(small_data_decision_tree))\n",
        "    print('Number of nodes that should be there: 7') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8lddEbCoM3f"
      },
      "source": [
        "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
        "\n",
        "```\n",
        "{'is_leaf': False,\n",
        "    'left': {'is_leaf': False,\n",
        "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
        "        'prediction': None,\n",
        "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
        "        'splitting_feature': 'grade_A'\n",
        "     },\n",
        "    'prediction': None,\n",
        "    'right': {'is_leaf': False,\n",
        "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
        "        'prediction': None,\n",
        "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
        "        'splitting_feature': 'grade_D'\n",
        "     },\n",
        "     'splitting_feature': 'term. 36 months'\n",
        "}```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTdds9xeoM3f"
      },
      "source": [
        "small_data_decision_tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E4rHcdMoM3f"
      },
      "source": [
        "### Making predictions with a weighted decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8_Lpq8ioM3g"
      },
      "source": [
        "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exYUKHHEoM3g"
      },
      "source": [
        "def classify(tree, x, annotate = False):   \n",
        "    # If the node is a leaf node.\n",
        "    if tree['is_leaf']:\n",
        "        if annotate: \n",
        "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
        "        return tree['prediction'] \n",
        "    else:\n",
        "        # Split on feature.\n",
        "        split_feature_value = x[tree['splitting_feature']]\n",
        "        if annotate: \n",
        "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
        "        if split_feature_value == 0:\n",
        "            return classify(tree['left'], x, annotate)\n",
        "        else:\n",
        "            return classify(tree['right'], x, annotate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRGCRG9CoM3g"
      },
      "source": [
        "### Evaluating the tree\n",
        "\n",
        "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
        "\n",
        "Again, recall that the **classification error** is defined as follows:\n",
        "$$\n",
        "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
        "$$\n",
        "\n",
        "The function called **evaluate_classification_error** takes in as input:\n",
        "1. `tree` (as described above)\n",
        "2. `data` (a dataframe)\n",
        "\n",
        "The function does not change because of adding data point weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9j5bC8KoM3g"
      },
      "source": [
        "def evaluate_classification_error(tree, data):\n",
        "    # Apply the classify(tree, x) to each row in your data\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "    \n",
        "    # Once you've made the predictions, calculate the classification error\n",
        "    return (prediction != data[target]).sum() / float(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie4oHq5LoM3g"
      },
      "source": [
        "evaluate_classification_error(small_data_decision_tree, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnlAxBRWoM3g"
      },
      "source": [
        "### Example: Training a weighted decision tree\n",
        "\n",
        "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
        "\n",
        "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
        "* 1 to the last 10 items \n",
        "* 1 to the first 10 items \n",
        "* and 0 to the rest. \n",
        "\n",
        "Let us fit a weighted decision tree with `max_depth = 2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofh6zAIZoM3g"
      },
      "source": [
        "# Assign weights\n",
        "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
        "\n",
        "# Train a weighted decision tree model.\n",
        "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
        "                         example_data_weights, max_depth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiT89z6NoM3g"
      },
      "source": [
        "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj2bMpn_oM3g"
      },
      "source": [
        "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
        "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtDJtR4oM3h"
      },
      "source": [
        "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyTsCG7oM3h"
      },
      "source": [
        "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfcCmF2hoM3h"
      },
      "source": [
        "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
        "\n",
        "So, what does this mean?\n",
        "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
        "* The points with zero weights are basically ignored during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgrPLHjCoM3h"
      },
      "source": [
        "# Implementing your own Adaboost (on decision stumps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoZTr7_ioM3h"
      },
      "source": [
        "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJnQffDooM3h"
      },
      "source": [
        "Recall from the lecture notes the procedure for Adaboost:\n",
        "\n",
        "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
        "\n",
        "2\\. For t = 1,...T:\n",
        "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
        "  * Compute coefficient $\\hat{w}_t$:\n",
        "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
        "  * Re-compute weights $\\alpha_j$:\n",
        "     $$\\alpha_j \\gets \\begin{cases}\n",
        "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
        "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
        "     \\end{cases}$$\n",
        "  * Normalize weights $\\alpha_j$:\n",
        "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
        "  \n",
        "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV41d71eoM3h"
      },
      "source": [
        "from math import log\n",
        "from math import exp\n",
        "\n",
        "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
        "    # start with unweighted data (uniformly weighted)\n",
        "    alpha =  np.array([1.]*len(data))\n",
        "    weights = []\n",
        "    tree_stumps = []\n",
        "    target_values = data[target]\n",
        "    \n",
        "    for t in range(num_tree_stumps):\n",
        "        print('=====================================================')\n",
        "        print('Adaboost Iteration %d' % t)\n",
        "        print('=====================================================')        \n",
        "        # Learn a weighted decision tree stump. Use max_depth=1\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # Make predictions\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # Produce a Boolean array indicating whether\n",
        "        # each data point was correctly classified\n",
        "        is_correct = predictions == target_values\n",
        "        is_wrong   = predictions != target_values\n",
        "        \n",
        "        # Compute weighted error\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # Compute model coefficient using weighted error\n",
        "        ## YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "        # Adjust weights on data point\n",
        "        ## YOUR CODE HERE\n",
        "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
        "        \n",
        "        # Scale alpha by multiplying by adjustment \n",
        "        # Then normalize data points weights\n",
        "        ## YOUR CODE HERE \n",
        "        ...\n",
        "    \n",
        "    return weights, tree_stumps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9uue4EKoM3h"
      },
      "source": [
        "### Checking your Adaboost code\n",
        "\n",
        "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
        "* `train_data`\n",
        "* `features`\n",
        "* `target`\n",
        "* `num_tree_stumps = 2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l1CtP2FToM3h"
      },
      "source": [
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvst51tqoM3h"
      },
      "source": [
        "def print_stump(tree):\n",
        "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
        "    if split_name is None:\n",
        "        print(\"(leaf, label: %s)\" % tree['prediction'])\n",
        "        return None\n",
        "    split_feature, split_value = split_name.split('_')\n",
        "    print('                       root')\n",
        "    print('         |---------------|----------------|')\n",
        "    print('         |                                |')\n",
        "    print('         |                                |')\n",
        "    print('         |                                |')\n",
        "    print('  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name))))\n",
        "    print('         |                                |')\n",
        "    print('         |                                |')\n",
        "    print('         |                                |')\n",
        "    print('    (%s)                 (%s)' \\\n",
        "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
        "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvtTHBB8oM3i"
      },
      "source": [
        "Here is what the first stump looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TYrf2xfoM3i"
      },
      "source": [
        "print_stump(tree_stumps[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opH5eDa4oM3i"
      },
      "source": [
        "Here is what the next stump looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQbcR1n7oM3i"
      },
      "source": [
        "print_stump(tree_stumps[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3g-o3lzoM3i"
      },
      "source": [
        "print(stump_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDw70ahCoM3i"
      },
      "source": [
        "If your Adaboost is correctly implemented, the following things should be true:\n",
        "\n",
        "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
        "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
        "* Weights should be approximately `[0.17, 0.18]` \n",
        "\n",
        "**Reminders**\n",
        "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
        "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
        "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-34Oj_doM3i"
      },
      "source": [
        "### Training a boosted ensemble of 10 stumps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oUMM7QWoM3i"
      },
      "source": [
        "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
        "* `train_data`\n",
        "* `features`\n",
        "* `target`\n",
        "* `num_tree_stumps = 10`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKrIwzS8oM3i"
      },
      "source": [
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
        "                                target, num_tree_stumps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkMv_jt1dO8y"
      },
      "source": [
        "### Plot the boosted stumps in the additive model\n",
        "\n",
        "The decision stumps picks a feature and a threshold, visualize them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeNYytx5dkMa"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkNoD_UvoM3j"
      },
      "source": [
        "## Making predictions\n",
        "\n",
        "Recall from the lecture that in order to make predictions, we use the following formula:\n",
        "$$\n",
        "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
        "$$\n",
        "\n",
        "We need to do the following things:\n",
        "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
        "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
        "- Sum the weighted predictions over each stump in the ensemble.\n",
        "\n",
        "Complete the following skeleton for making predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9S4UTFIoM3j"
      },
      "source": [
        "def predict_adaboost(stump_weights, tree_stumps, data):\n",
        "    scores = np.array([0.]*len(data))\n",
        "    \n",
        "    for i, tree_stump in enumerate(tree_stumps):\n",
        "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
        "        \n",
        "        # Accumulate predictions on scaores array\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        \n",
        "    return scores.apply(lambda score : +1 if score > 0 else -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yltvmKauoM3j"
      },
      "source": [
        "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(test_data[target], predictions)\n",
        "print('Accuracy of 10-component ensemble = %s' % accuracy) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU7snIBqoM3j"
      },
      "source": [
        "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiz9BkA0oM3j"
      },
      "source": [
        "stump_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbxmBB78oM3j"
      },
      "source": [
        "**Question** i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
        "\n",
        "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OffpbikcoM3j"
      },
      "source": [
        "# Performance plots\n",
        "\n",
        "In this section, we will try to reproduce some performance plots.\n",
        "\n",
        "### How does accuracy change with adding stumps to the ensemble?\n",
        "\n",
        "We will now train an ensemble with:\n",
        "* `train_data`\n",
        "* `features`\n",
        "* `target`\n",
        "* `num_tree_stumps = 30`\n",
        "\n",
        "Once we are done with this, we will then do the following:\n",
        "* Compute the classification error at the end of each iteration.\n",
        "* Plot a curve of classification error vs iteration.\n",
        "\n",
        "First, lets train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DfpOHylUoM3j"
      },
      "source": [
        "# this may take a while... \n",
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
        "                                 features, target, num_tree_stumps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aYX1Bf5oM3j"
      },
      "source": [
        "### Computing training error at the end of each iteration\n",
        "\n",
        "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myoLZ4L9oM3j"
      },
      "source": [
        "error_all = []\n",
        "for n in range(1, 31):\n",
        "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
        "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
        "    error_all.append(error)\n",
        "    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxw0ZdlHoM3j"
      },
      "source": [
        "### Visualizing training error vs number of iterations\n",
        "\n",
        "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Te1Mms-oM3k"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = 7, 5\n",
        "plt.plot(list(range(1,31)), error_all, '-', linewidth=4.0, label='Training error')\n",
        "plt.title('Performance of Adaboost ensemble')\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Classification error')\n",
        "plt.legend(loc='best', prop={'size':15})\n",
        "\n",
        "plt.rcParams.update({'font.size': 16})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1mZai_6oM3k"
      },
      "source": [
        "### Evaluation on the test data\n",
        "\n",
        "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Uo8rKuoM3k"
      },
      "source": [
        "test_error_all = []\n",
        "for n in range(1, 31):\n",
        "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
        "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
        "    test_error_all.append(error)\n",
        "    print(\"Iteration %s, test error = %s\" % (n, test_error_all[n-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De4zPXJzoM3k"
      },
      "source": [
        "### Visualize both the training and test errors\n",
        "\n",
        "Now, let us plot the training & test error with the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkxNge6HoM3k"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = 7, 5\n",
        "plt.plot(list(range(1,31)), error_all, '-', linewidth=4.0, label='Training error')\n",
        "plt.plot(list(range(1,31)), test_error_all, '-', linewidth=4.0, label='Test error')\n",
        "\n",
        "plt.title('Performance of Adaboost ensemble')\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Classification error')\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "plt.legend(loc='best', prop={'size':15})\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lafYFpn0oM3k"
      },
      "source": [
        "**Question** ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4AeYzVfoM3k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}